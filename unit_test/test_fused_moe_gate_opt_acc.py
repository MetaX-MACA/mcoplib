import torch
import torch.nn.functional as F
from mcoplib import op as ops


def cosine_similarity(a, b):
    """
    è®¡ç®—ä¸¤ä¸ªtensorçš„ä½™å¼¦ç›¸ä¼¼åº¦

    Args:
        a: tensor, ä»»æ„å½¢çŠ¶
        b: tensor, å¿…é¡»ä¸aå½¢çŠ¶ç›¸åŒ

    Returns:
        float: ä½™å¼¦ç›¸ä¼¼åº¦ï¼ŒèŒƒå›´[-1, 1]
                - 1.0 è¡¨ç¤ºå®Œå…¨ç›¸åŒ
                - 0.0 è¡¨ç¤ºæ­£äº¤
                - -1.0 è¡¨ç¤ºå®Œå…¨ç›¸å
                è¶Šæ¥è¿‘1è¡¨ç¤ºè¶Šç›¸ä¼¼
    """
    # å±•å¹³ä¸ºä¸€ç»´å‘é‡
    a_flat = a.flatten().float()
    b_flat = b.flatten().float()

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦: cos(Î¸) = (aÂ·b) / (||a|| * ||b||)
    dot_product = torch.sum(a_flat * b_flat)
    norm_a = torch.norm(a_flat)
    norm_b = torch.norm(b_flat)

    # é¿å…é™¤é›¶é”™è¯¯
    norm_a = norm_a if norm_a.item() > 1e-8 else torch.tensor(1.0, device=a.device)
    norm_b = norm_b if norm_b.item() > 1e-8 else torch.tensor(1.0, device=a.device)

    cosine_sim = dot_product / (norm_a * norm_b)
    return cosine_sim.item()


def reference_moe_gate(gating_outputs, correction_bias, topk, num_expert_group,
                        topk_group, num_fused_shared_experts=None,
                        routed_scaling_factor=None):
    
    batch_size = gating_outputs.shape[0]
    num_experts = gating_outputs.shape[1]
    device = gating_outputs.device

    # Step 1: Sigmoidæ¿€æ´»
    sigmoid_output = torch.sigmoid(gating_outputs.float())

    # Step 2: åŠ biasï¼ˆç”¨äºä¸“å®¶é€‰æ‹©ï¼‰
    bias_output = sigmoid_output + correction_bias.float()

    # Step 3: ç¡®å®šå®é™…éœ€è¦é€‰æ‹©çš„topkæ•°é‡ï¼ˆæ’é™¤å…±äº«ä¸“å®¶ï¼‰
    if num_fused_shared_experts is None or num_fused_shared_experts == 0:
        topk_excluding_shared = topk
        has_shared = False
    else:
        topk_excluding_shared = topk - num_fused_shared_experts
        has_shared = True

    # Step 4: åŸºäºbiasåçš„å€¼é€‰æ‹©topkä¸“å®¶
    if num_expert_group == 1:
        # å•ç»„æ¨¡å¼ï¼šç›´æ¥é€‰æ‹©topk
        topk_values, topk_indices = torch.topk(bias_output, k=topk_excluding_shared, dim=1)
    else:
        # å¤šç»„æ¨¡å¼ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥é€‰æ‹©topk
        # å®é™…çš„CUDAå®ç°ä¼šå…ˆé€‰æ‹©topk_groupä¸ªç»„ï¼Œå†ä»è¿™äº›ç»„ä¸­é€‰æ‹©
        topk_values, topk_indices = torch.topk(bias_output, k=topk_excluding_shared, dim=1)

    # Step 5: è·å–sigmoidåçš„æƒé‡å€¼ï¼ˆæ³¨æ„ï¼šä½¿ç”¨sigmoidåçš„å€¼ï¼Œä¸æ˜¯biasåçš„å€¼ï¼‰
    selected_weights = torch.gather(sigmoid_output, 1, topk_indices)

    # Step 6: å¤„ç†å…±äº«ä¸“å®¶
    if has_shared:
        # è®¡ç®—æ‰€æœ‰é€‰ä¸­ä¸“å®¶æƒé‡çš„å’Œ
        weight_sum = selected_weights.sum(dim=1, keepdim=True)  # [batch_size, 1]

        # å…±äº«ä¸“å®¶çš„æƒé‡ = sum_of_weights / routed_scaling_factor
        scale = routed_scaling_factor if routed_scaling_factor is not None else 1.0
        shared_weight = weight_sum / scale  # [batch_size, 1]

        # æ·»åŠ å…±äº«ä¸“å®¶
        for i in range(num_fused_shared_experts):
            # å…±äº«ä¸“å®¶çš„ç´¢å¼• = num_experts + iï¼ˆè¶…å‡ºå®é™…ä¸“å®¶èŒƒå›´ï¼‰
            shared_indices = torch.full((batch_size, 1), num_experts + i,
                                        dtype=torch.long, device=device)

            # æ‹¼æ¥ç´¢å¼•å’Œæƒé‡
            topk_indices = torch.cat([topk_indices, shared_indices], dim=1)
            selected_weights = torch.cat([selected_weights, shared_weight], dim=1)

    # Step 7: å½’ä¸€åŒ–æƒé‡
    # æ³¨æ„ï¼šå¦‚æœæœ‰å…±äº«ä¸“å®¶ï¼Œåªå½’ä¸€åŒ–è·¯ç”±ä¸“å®¶ï¼Œä¸åŒ…æ‹¬å…±äº«ä¸“å®¶
    # è¿™ä¸biased_grouped_topkçš„é€»è¾‘ä¸€è‡´
    if has_shared:
        # åªè®¡ç®—è·¯ç”±ä¸“å®¶çš„å’Œä½œä¸ºå½’ä¸€åŒ–åŸºæ•°
        weight_sum = selected_weights[:, :-1].sum(dim=1, keepdim=True)
    else:
        # æ— å…±äº«ä¸“å®¶ï¼Œè®¡ç®—æ‰€æœ‰ä¸“å®¶çš„å’Œ
        weight_sum = selected_weights.sum(dim=1, keepdim=True)
    normalized_weights = selected_weights / (weight_sum + 1e-8)  # æ·»åŠ å°å€¼é¿å…é™¤é›¶

    return normalized_weights, topk_indices

def biased_grouped_topk_impl(
    hidden_states: torch.Tensor,
    gating_output: torch.Tensor,
    correction_bias: torch.Tensor,
    topk: int,
    renormalize: bool,
    num_expert_group: Optional[int] = None,
    topk_group: Optional[int] = None,
    num_fused_shared_experts: int = 0,
    routed_scaling_factor: Optional[float] = None,
    # num_token_non_padded: Optional[torch.Tensor] = None,
    # expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    apply_routed_scaling_factor_on_output: Optional[bool] = False,
):
    assert hidden_states.shape[0] == gating_output.shape[0], "Number of tokens mismatch"

    scores = gating_output.sigmoid()
    num_token = scores.shape[0]
    num_experts = scores.shape[1]
    scores_for_choice = scores.view(num_token, -1) + correction_bias.unsqueeze(0)
    group_scores = (
        scores_for_choice.view(num_token, num_expert_group, -1)
        .topk(2, dim=-1)[0]
        .sum(dim=-1)
    ) # [n, n_group]
    group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[
        1
    ] # [n, top_k_group]
    group_mask = torch.zeros_like(group_scores) # [n, n_group]
    group_mask.scatter_(1, group_idx, 1) # [n, n_group]
    score_mask = (
        group_mask.unsqueeze(-1)
        .expand(num_token, num_expert_group, scores.shape[-1] // num_expert_group)
        .reshape(num_token, -1)
    ) # [n, e]
    tmp_scores = scores_for_choice.masked_fill(
        ~score_mask.bool(), float("-inf")
    ) # [n, e]
    # TODO: NPU can't support directly evaluating a comparison for now
    _, topk_ids = torch.topk(
        tmp_scores,
        k=topk,
        dim=-1,
        sorted=(True if num_fused_shared_experts > 0 else False),
    )
    topk_weights = scores.gather(1, topk_ids)

    if num_fused_shared_experts:
        topk_ids[:, -1] = torch.randint(
            low=num_experts,
            high=num_experts + num_fused_shared_experts,
            size=(topk_ids.size(0),),
            dtype=topk_ids.dtype,
            device=topk_ids.device,
        )
        topk_weights[:, -1] = topk_weights[:, :-1].sum(dim=-1) / routed_scaling_factor

    if renormalize:
        topk_weights_sum = (
            topk_weights.sum(dim=-1, keepdim=True)
            if num_fused_shared_experts == 0
            else topk_weights[:, :-1].sum(dim=-1, keepdim=True)
        )
        topk_weights = topk_weights / topk_weights_sum
        if apply_routed_scaling_factor_on_output:
             topk_weights *= routed_scaling_factor

    topk_weights, topk_ids = topk_weights.to(torch.float32), topk_ids.to(torch.int32)
    # topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)
    # _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)
    return topk_weights, topk_ids

def verify_accuracy(output_weights, output_indices, ref_weights, ref_indices,
                    tolerance=0.00001, test_name=""):
    """
    éªŒè¯è¾“å‡ºç²¾åº¦

    Args:
        output_weights: ç®—å­è¾“å‡ºçš„æƒé‡
        output_indices: ç®—å­è¾“å‡ºçš„ä¸“å®¶ç´¢å¼•
        ref_weights: å‚è€ƒå®ç°çš„æƒé‡
        ref_indices: å‚è€ƒå®ç°çš„ä¸“å®¶ç´¢å¼•
        tolerance: å®¹å·®ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ä¸1.0çš„æœ€å¤§å·®è·ï¼‰
        test_name: æµ‹è¯•åç§°
    """
    # 1. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    weight_similarity = cosine_similarity(output_weights, ref_weights)
    similarity_diff = abs(1.0 - weight_similarity)

    print(f"  {test_name} - Weight Cosine Similarity: {weight_similarity:.10f}")
    print(f"  {test_name} - Difference from perfect (1.0): {similarity_diff:.10f}")

    # 2. éªŒè¯ä½™å¼¦ç›¸ä¼¼åº¦
    if similarity_diff >= tolerance:
        # æ‰“å°è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        print(f"\n  ğŸ“Š è¯¦ç»†è°ƒè¯•ä¿¡æ¯ï¼š")
        batch_size = output_indices.shape[0]
        topk = output_indices.shape[1]

        for b in range(batch_size):
            print(f"\n  Batch {b}:")
            print(f"    CUDA ç´¢å¼•: {output_indices[b].cpu().numpy()}")
            print(f"    Ref  ç´¢å¼•: {ref_indices[b].cpu().numpy()}")

            indices_match = torch.equal(output_indices[b], ref_indices[b])
            if not indices_match:
                print(f"    âš ï¸  ç´¢å¼•ä¸åŒ¹é…ï¼")

            cuda_w = output_weights[b].cpu()
            ref_w = ref_weights[b].cpu()
            print(f"    CUDA æƒé‡: {cuda_w.numpy()}")
            print(f"    Ref  æƒé‡: {ref_w.numpy()}")
            print(f"    æƒé‡å·®å¼‚: {(cuda_w - ref_w).numpy()}")

            # å•ä¸ªbatchçš„ä½™å¼¦ç›¸ä¼¼åº¦
            batch_cosine = torch.cosine_similarity(
                cuda_w.unsqueeze(0), ref_w.unsqueeze(0)
            ).item()
            print(f"    Batchä½™å¼¦ç›¸ä¼¼åº¦: {batch_cosine:.10f}")

        raise AssertionError(
            f"{test_name} - Accuracy check failed!\n"
            f"  Cosine similarity = {weight_similarity:.10f}\n"
            f"  Difference from 1.0 = {similarity_diff:.10f} >= {tolerance}\n"
            f"  This indicates the outputs do not match the reference implementation."
        )

    # 3. éªŒè¯ä¸“å®¶ç´¢å¼•ï¼ˆå…è®¸é¡ºåºä¸åŒï¼Œä½†é›†åˆå¿…é¡»ç›¸åŒï¼‰
    batch_size = output_indices.shape[0]
    for i in range(batch_size):
        pred_set = set(output_indices[i].cpu().numpy())
        ref_set = set(ref_indices[i].cpu().numpy())

        if pred_set != ref_set:
            raise AssertionError(
                f"{test_name} - Expert indices mismatch at batch {i}!\n"
                f"  Predicted: {sorted(pred_set)}\n"
                f"  Reference: {sorted(ref_set)}\n"
                f"  Missing: {ref_set - pred_set}\n"
                f"  Extra: {pred_set - ref_set}"
            )

    print(f"  âœ… {test_name} - Accuracy verification passed!")


def test_160_experts_no_shared():
    """æµ‹è¯•160ä¸“å®¶ï¼Œæ— å…±äº«ä¸“å®¶ï¼Œå¹¶éªŒè¯ç²¾åº¦"""
    print("\n" + "="*70)
    print("[Test 1] Testing 160 experts (no shared experts)")
    print("="*70)

    batch_size = 4
    num_experts = 160
    topk = 8

    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§
    torch.manual_seed(1234)

    # åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆä¿®æ­£ï¼šä½¿ç”¨deviceå‚æ•°ï¼‰
    gating_outputs = torch.randn(batch_size, num_experts, dtype=torch.bfloat16, device='cuda')
    correction_bias = torch.randn(num_experts, dtype=torch.bfloat16, device='cuda')
    out_routing_weights = torch.empty(batch_size, topk, dtype=torch.float, device='cuda')
    out_selected_experts = torch.empty(batch_size, topk, dtype=torch.int32, device='cuda')

    print(f"  Configuration:")
    print(f"    Batch size: {batch_size}")
    print(f"    Num experts: {num_experts}")
    print(f"    TopK: {topk}")
    print(f"    Expert groups: 1")
    print(f"    Shared experts: 0")

    # è°ƒç”¨ç®—å­
    ret = ops.fused_moe_gate_opt(
        gating_outputs,
        correction_bias,
        out_routing_weights,
        out_selected_experts,
        topk=topk,
        renormalize=True,
        num_expert_group=1,
        topk_group=1,
        num_fused_shared_experts=None,
        routed_scaling_factor=None
    )

    # åŸºæœ¬æ–­è¨€
    assert ret == 0, f"fused_moe_gate_opt returned error code {ret}"
    assert out_selected_experts.shape == (batch_size, topk), \
        f"Output shape mismatch: {out_selected_experts.shape} != ({batch_size}, {topk})"
    assert torch.all(out_selected_experts >= 0), "Expert indices should be non-negative"
    assert torch.all(out_selected_experts < num_experts), \
        f"Expert indices should be < {num_experts}"

    # éªŒè¯æƒé‡å½’ä¸€åŒ–ï¼ˆå’Œåº”è¯¥ä¸º1ï¼‰
    # æ³¨æ„ï¼šç”±äºbfloat16ç²¾åº¦çš„é™åˆ¶ï¼Œæƒé‡å’Œå¯èƒ½ä¸å®Œå…¨ç­‰äº1.0
    # bfloat16åªæœ‰7ä½æœ‰æ•ˆç²¾åº¦ï¼Œåœ¨å¤šæ¬¡ç±»å‹è½¬æ¢å’Œé™¤æ³•è¿ç®—åä¼šæœ‰ç´¯ç§¯è¯¯å·®
    # æ­£å¸¸è¯¯å·®èŒƒå›´åº”è¯¥åœ¨ Â±0.005 ä»¥å†…
    weight_sums = out_routing_weights.sum(dim=1)
    normalization_tolerance = 5e-3  # æ”¾å®½å®¹å·®åˆ°0.005ï¼Œé€‚åº”bfloat16ç²¾åº¦
    assert torch.all(torch.abs(weight_sums - 1.0) < normalization_tolerance), \
        f"Weights should sum to 1.0 (within tolerance {normalization_tolerance}), " \
        f"got sums in range [{weight_sums.min():.6f}, {weight_sums.max():.6f}]"

    print(f"  âœ“ Basic checks passed")

    # è®¡ç®—å‚è€ƒå®ç°
    ref_weights, ref_indices = biased_grouped_topk_impl(
        gating_outputs, correction_bias, topk,
        num_expert_group=1, topk_group=1,
        num_fused_shared_experts=None,
        routed_scaling_factor=None
    )

    # ç²¾åº¦éªŒè¯
    verify_accuracy(
        out_routing_weights, out_selected_experts,
        ref_weights, ref_indices,
        tolerance=0.00001,
        test_name="Test 1 (160 experts, no shared)"
    )

    print("âœ… Test 1 passed!")


def test_160_experts_with_shared():
    """æµ‹è¯•160ä¸“å®¶ï¼Œ1ä¸ªå…±äº«ä¸“å®¶ï¼Œå¹¶éªŒè¯ç²¾åº¦"""
    print("\n" + "="*70)
    print("[Test 2] Testing 160 experts (with 1 shared expert)")
    print("="*70)

    batch_size = 4
    num_experts = 160
    topk = 9  # 8 + 1 shared
    num_shared = 1
    routed_scaling_factor = 2.0

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(5678)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    gating_outputs = torch.randn(batch_size, num_experts, dtype=torch.bfloat16, device='cuda')
    correction_bias = torch.randn(num_experts, dtype=torch.bfloat16, device='cuda')
    out_routing_weights = torch.empty(batch_size, topk, dtype=torch.float, device='cuda')
    out_selected_experts = torch.empty(batch_size, topk, dtype=torch.int32, device='cuda')

    print(f"  Configuration:")
    print(f"    Batch size: {batch_size}")
    print(f"    Num experts: {num_experts}")
    print(f"    TopK: {topk}")
    print(f"    Expert groups: 1")
    print(f"    Shared experts: {num_shared}")
    print(f"    Routed scaling factor: {routed_scaling_factor}")

    # è°ƒç”¨ç®—å­
    ret = ops.fused_moe_gate_opt(
        gating_outputs,
        correction_bias,
        out_routing_weights,
        out_selected_experts,
        topk=topk,
        renormalize=True,
        num_expert_group=1,
        topk_group=1,
        num_fused_shared_experts=num_shared,
        routed_scaling_factor=routed_scaling_factor
    )

    # åŸºæœ¬æ–­è¨€
    assert ret == 0, f"fused_moe_gate_opt returned error code {ret}"

    # æ£€æŸ¥æœ€åä¸€ä¸ªä¸“å®¶æ˜¯å…±äº«ä¸“å®¶ï¼ˆç´¢å¼•160ï¼‰
    assert torch.all(out_selected_experts[:, -1] == num_experts), \
        f"Last expert should be shared expert with index {num_experts}, " \
        f"but got indices in range [{out_selected_experts[:, -1].min()}, {out_selected_experts[:, -1].max()}]"

    # éªŒè¯æƒé‡å½’ä¸€åŒ–
    # æ³¨æ„ï¼šç”±äºbfloat16ç²¾åº¦çš„é™åˆ¶ï¼Œæƒé‡å’Œå¯èƒ½ä¸å®Œå…¨ç­‰äº1.0
    # å¦å¤–ï¼Œå½“æœ‰å…±äº«ä¸“å®¶æ—¶ï¼Œæƒé‡å’Œ = 1.0 + (1.0 / routed_scaling_factor)
    # ä¾‹å¦‚ï¼šrouted_scaling_factor=2.0æ—¶ï¼Œæƒé‡å’Œ = 1.0 + 0.5 = 1.5
    weight_sums = out_routing_weights.sum(dim=1)
    expected_sum = 1.0 + (1.0 / routed_scaling_factor)  # 1.0 + shared_weight
    normalization_tolerance = 5e-3  # æ”¾å®½å®¹å·®åˆ°0.005ï¼Œé€‚åº”bfloat16ç²¾åº¦
    assert torch.all(torch.abs(weight_sums - expected_sum) < normalization_tolerance), \
        f"Weights should sum to {expected_sum:.3f} (within tolerance {normalization_tolerance}), " \
        f"got sums in range [{weight_sums.min():.6f}, {weight_sums.max():.6f}]"

    print(f"  âœ“ Basic checks passed")
    print(f"  âœ“ Shared expert index verified: all = {num_experts}")
    print(f"  âœ“ Weight sum verified: ~{expected_sum:.3f}")

    # è®¡ç®—å‚è€ƒå®ç°
    ref_weights, ref_indices = biased_grouped_topk_impl(
        gating_outputs, correction_bias, topk,
        num_expert_group=1, topk_group=1,
        num_fused_shared_experts=num_shared,
        routed_scaling_factor=routed_scaling_factor
    )

    # ç²¾åº¦éªŒè¯
    verify_accuracy(
        out_routing_weights, out_selected_experts,
        ref_weights, ref_indices,
        tolerance=0.00001,
        test_name="Test 2 (160 experts, with shared)"
    )

    print("âœ… Test 2 passed!")


def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    print("\n" + "="*70)
    print("[Test 3] Testing edge cases")
    print("="*70)

    # æµ‹è¯•1: å°batch size
    print("\n  [Edge Case 1] Small batch size (batch=1)")
    torch.manual_seed(9012)
    gating_outputs = torch.randn(1, 160, dtype=torch.bfloat16, device='cuda')
    correction_bias = torch.randn(160, dtype=torch.bfloat16, device='cuda')
    out_weights = torch.empty(1, 8, dtype=torch.float, device='cuda')
    out_indices = torch.empty(1, 8, dtype=torch.int32, device='cuda')

    ret = ops.fused_moe_gate_opt(
        gating_outputs, correction_bias, out_weights, out_indices,
        topk=8, renormalize=True, num_expert_group=1, topk_group=1,
        num_fused_shared_experts=None, routed_scaling_factor=None
    )
    assert ret == 0, "Failed for batch_size=1"
    print("  âœ“ Batch size 1 passed")

    # æµ‹è¯•2: å¤§batch size
    print("\n  [Edge Case 2] Large batch size (batch=128)")
    torch.manual_seed(3456)
    gating_outputs = torch.randn(128, 160, dtype=torch.bfloat16, device='cuda')
    correction_bias = torch.randn(160, dtype=torch.bfloat16, device='cuda')
    out_weights = torch.empty(128, 8, dtype=torch.float, device='cuda')
    out_indices = torch.empty(128, 8, dtype=torch.int32, device='cuda')

    ret = ops.fused_moe_gate_opt(
        gating_outputs, correction_bias, out_weights, out_indices,
        topk=8, renormalize=True, num_expert_group=1, topk_group=1,
        num_fused_shared_experts=None, routed_scaling_factor=None
    )
    assert ret == 0, "Failed for batch_size=128"
    print("  âœ“ Batch size 128 passed")

    # æµ‹è¯•3: éªŒè¯æ‰€æœ‰æƒé‡éƒ½éè´Ÿ
    assert torch.all(out_weights >= 0), "All weights should be non-negative"
    print("  âœ“ All weights are non-negative")

    # æµ‹è¯•4: éªŒè¯æƒé‡åœ¨åˆç†èŒƒå›´å†…
    assert torch.all(out_weights <= 1.0), "All weights should be <= 1.0"
    print("  âœ“ All weights are <= 1.0")

    print("\nâœ… Test 3 (edge cases) passed!")


if __name__ == "__main__":
    print("\n" + "="*70)
    print("FUSED MOE GATE - ACCURACY VERIFICATION TEST SUITE")
    print("="*70)
    print("\nThis test suite verifies the accuracy of the fused_moe_gate_opt")
    print("operator by comparing its output against a reference implementation.")
    print("\nAccuracy metric: Cosine Similarity")
    print("  - Perfect match: similarity = 1.0")
    print("  - Tolerance: 1.0 - similarity < 0.00001")
    print("  - If difference >= 0.00001, test FAILS")

    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_160_experts_no_shared()
        test_160_experts_with_shared()
        test_edge_cases()

        # æ‰€æœ‰æµ‹è¯•é€šè¿‡
        print("\n" + "="*70)
        print("ALL TESTS PASSED! âœ…âœ…âœ…")
        print("="*70)
        print("\nThe fused_moe_gate_opt operator has been verified to be accurate")
        print("within the specified tolerance.")
        print("="*70)

    except Exception as e:
        print("\n" + "="*70)
        print("TEST FAILED! âŒ")
        print("="*70)
        print(f"\nError: {str(e)}")
        print("\nPlease check:")
        print("  1. Is the CUDA device available?")
        print("  2. Is mcoplib properly installed?")
        print("  3. Are the kernel implementations correct?")
        print("="*70)
        raise